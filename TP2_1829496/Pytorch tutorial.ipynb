{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction a PyTorch\n",
    "\n",
    "Based on https://github.com/mila-udem/welcome_tutorials/tree/master/pytorch By Sandeep Subramanian, https://github.com/jcjohnson/pytorch-examples by Justin Johnson and official documentation http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "Tensors are similar to numpy array, but they can also be used on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "# convert numpy array to tensor\n",
    "torch.from_numpy(np.array([1,2,3,4]))  # LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.array([1.,2,3,4]))  # Double Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([1,2,3,4])\n",
    "torch.Tensor([1,2,3,4])  # float tensor by default, can change it with torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1,2,3,4])\n",
    "# HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.4142, 1.7321, 2.0000])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sqrt()  # similar functionality as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "x = x.view(-1, 1)  # the torch reshape function\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 2.],\n",
       "        [3., 3.],\n",
       "        [4., 4.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x, x], 1)  # concatenate 2 vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd provides automatic differentiation on all operations perform on tensors. To be able to use Autograd, you must wrap your tensors in a Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], requires_grad=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(x, requires_grad=False)\n",
    "w = Variable(torch.ones(4, 1), requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-2cfa5d8249c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dot: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "z = torch.dot(w, x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([w], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-d1156426382c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "z.backward()\n",
    "print(w.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9900\n",
      " 0.9800\n",
      " 0.9700\n",
      " 0.9600\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-2cd26b6fc9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dot: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "z = torch.dot(w, x)\n",
    "target = Variable(torch.zeros(1))\n",
    "optimizer.zero_grad()\n",
    "loss = loss_fn(z, target)\n",
    "loss.backward()  # retain_graph=True, if you need to call loss.backward() again without optimizer.step()\n",
    "optimizer.step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "\n",
    "based on MNIST tutorial: https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from fashion import FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "# torch.cuda.init()\n",
    "# torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "valid_data = FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.random.choice(train_data.train_data.shape[0], 54000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.train_data = train_data.train_data[train_idx, :]\n",
    "train_data.train_labels = train_data.train_labels[torch.from_numpy(train_idx).type(torch.LongTensor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(60000)\n",
    "mask[train_idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.train_data = valid_data.train_data[torch.from_numpy(np.argwhere(mask)), :].squeeze()\n",
    "valid_data.train_labels = valid_data.train_labels[torch.from_numpy(mask).type(torch.ByteTensor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "test_batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    FashionMNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8bec4a518>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_loader.dataset.train_data[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8becde0f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_loader.dataset.train_data[10].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FcNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        batch_size = image.size()[0]\n",
    "        x = image.view(batch_size, -1)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully-connected single layer\n",
    "class FcNetwork1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        batch_size = image.size()[0]\n",
    "        x = image.view(batch_size, -1)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional N dual layers\n",
    "class CNNetworkMultiLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = Variable(data, requires_grad=False).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "#         with torch.no_grad():\n",
    "        data, target = Variable(data, requires_grad=False), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # calls the forward function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def valid(model, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "#         data, target = Variable(data, requires_grad=False).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "\n",
    "        data, target = Variable(data, requires_grad=False), Variable(target)\n",
    "        output = model(data)\n",
    "        valid_loss += F.nll_loss(output, target, reduction='sum').data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    print('\\n' + \"valid\" + ' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        valid_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))\n",
    "    \n",
    "    return 100. * correct / len(valid_loader.dataset), valid_loss\n",
    "\n",
    "    \n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "#         data, target = Variable(data, requires_grad=False).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "#         with torch.no_grad():\n",
    "        data, target = Variable(data, requires_grad=False), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\n' + \"test\" + ' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    \n",
    "def experiment(model, epochs=30, lr=0.001):\n",
    "    best_precision = 0\n",
    "    losses_train = []\n",
    "    losses_validation = []\n",
    "#     best_model = FcNetwork().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model, train_loss = train(model, train_loader, optimizer)\n",
    "        precision, validation_loss = valid(model, valid_loader)\n",
    "        losses_train.append(train_loss)\n",
    "        losses_validation.append(validation_loss)\n",
    "        if precision > best_precision:\n",
    "            best_precision = precision\n",
    "            best_model = model\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(losses_train, 'b', label=\"Train\")\n",
    "    ax1.plot(losses_validation, 'm', label=\"Validation\")\n",
    "    ax1.set_title('Courbes d\\'apprentissage')\n",
    "    ax1.set_ylabel('Log n√©gatif de vraisemblance moyenne')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, best_precision\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid set: Average loss: 0.3548, Accuracy: 5245/6000 (87%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.2930, Accuracy: 5353/6000 (89%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-62bba41fa5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCNNetworkMultiLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# add your models in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     model.cuda()  # if you have access to a gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_precision\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbest_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d9b6091513b5>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mlosses_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d9b6091513b5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#         data, target = Variable(data, requires_grad=False).cuda(), Variable(target).cuda() # if you have access to a gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         with torch.no_grad():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2480\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2428\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m             im = im._new(\n\u001b[0;32m-> 2430\u001b[0;31m                 \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2431\u001b[0m                 )\n\u001b[1;32m   2432\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# FIXME: take \"new\" parameters / other image?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# FIXME: turn mode and size into delegating properties?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_precision = 0\n",
    "# device = torch.device(\"cpu\")\n",
    "# best_model = FcNetwork().to(device)\n",
    "\n",
    "\n",
    "for model in [CNNetworkMultiLayer()]:  # add your models in the list\n",
    "#     model.cuda()  # if you have access to a gpu\n",
    "    model, precision = experiment(model)\n",
    "    if precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_model = model\n",
    "\n",
    "test(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
