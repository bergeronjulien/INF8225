{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout is best !!!!111!1!1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from fashion import FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe022b1fd68>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAE2pJREFUeJzt3V2MXOV5B/D/M7Mzu94Pr3e99sY2xiYucUpoIXTltA2tqGgIoEiGG4QvkCOhOBehaqRILaUX5a6oahJxUQU5xYqpEkKlBEFTlASsSiRpRL1Qx8YhxECX4MVem3jN2vs5H08v9hAtZs/zLPN1Zv38f5Ll2XnnnPPu7P73zMxz3vcVVQURxZPLugNElA2Gnygohp8oKIafKCiGnygohp8oKIafKCiGnygohp8oqI5WHqwondqFnlYeclWQYtF+QKlkNuvV6T/Gno4Fc9vz82vM9mpVzPZ83r5CdH3ndGrb2fNrzW2L4+nbAoDk82a7Vipm++VoDtNY0Hn7h5aoK/wiciuAhwHkAfyrqj5kPb4LPfiU3FzPIS9LHVuuNNurp8+Y7eVHNqS2fWr9mLntU//3B2b7zHSn2b62b9Zsv2fH/6S2PfIfnzW3vervfm625/sHzPbK5KTZfjl6QQ+t+LE1v+wXkTyAfwFwG4BrAOwRkWtq3R8RtVY97/l3AXhNVd9Q1QUA3wWwuzHdIqJmqyf8WwC8teTrk8l97yMi+0RkVERGS5iv43BE1EhN/7RfVfer6oiqjhRgv38kotapJ/zjALYu+fqK5D4iWgXqCf9hAFeLyFUiUgRwN4CnG9MtImq2mkt9qloWkfsA/AiLpb4Dqnq8YT1rM/m16TXp0nU7zG0rnfbfWLuKD3Stsd8uFT+fXm77zt/8qbntI7cdMNtv6bZ793rpor39T/4qtW3ngQlz2+offcJsn95sXzNSuFhObSuePG9uWznxhtl+Oairzq+qzwB4pkF9IaIW4uW9REEx/ERBMfxEQTH8REEx/ERBMfxEQUkrV+xZK4ParkN6c319Zvvsn308ta1rYsbe94w9ph4552/wgnMlwFR6rb0yYQ8HzlJ+/aD9gIH+uvavPV2pbbNX9JrbFiftn5n89y9q6lOzvaCHMKXnVjSen2d+oqAYfqKgGH6ioBh+oqAYfqKgGH6ioFo6dXc7y623Z4Jd83b6NNK5d+0pprXDnmIamj70FADgTFGNtellq9xH1pubyoJz7LIz/bVXpqxHtWq3O2Xq3FR6Cbb71Tlz2/ltdhmy2JVeRgSA6py9/3bAMz9RUAw/UVAMP1FQDD9RUAw/UVAMP1FQDD9RUKzzJ6r99jTQ5d70ZbS9JzH32ymzXbucJbpzzgjNjkJqk5SdWrnHu0bBY1wHoGJ/X1JyrkFwnhd1lg83j12xt5Wtm+0drIKpv3nmJwqK4ScKiuEnCorhJwqK4ScKiuEnCorhJwqqrjq/iIwBuACgAqCsqiON6FQmnJpzYTJ9GexTN9lj5jf/p72MtVy0p/6uTtrLSZtjx71aunGNwEpo2ZlWvI6p4XPd3XZ7f/qy6Z7S9mF73wv2PAbVtWtqPna7aMRFPn+hqu80YD9E1EJ82U8UVL3hVwA/FpEXRWRfIzpERK1R78v+G1V1XEQ2AnhWRH6lqs8vfUDyR2EfAHTBfg9HRK1T15lfVceT/88AeBLArmUes19VR1R1pIDOeg5HRA1Uc/hFpEdE+t67DeAWAC83qmNE1Fz1vOwfBvCkLJaSOgB8R1V/2JBeEVHT1Rx+VX0DwHUN7EumNO/Uw2fT6769bztz2zu1dl1nLw9ecWrSk7+f/llKz4QzJt4pwxfP20tVnxmx50Eovpt+gIV+Z54Cp2+bf3jabC8PpT+v53fanz+t/1/72opKjz0Hw4rWyM4YS31EQTH8REEx/ERBMfxEQTH8REEx/ERBhZm6WwpOaWbeLtfNbl+X2tb3a7sspEV72Oz4P9o/hpkZu3BUnk8vx70z60y97dSkcr32+WFwwB7QOTmVXlIrz9t9K3bbw4X1J/Yy2fND6VeU5uftOqJ22N933VOitwGe+YmCYviJgmL4iYJi+ImCYviJgmL4iYJi+ImCClPnz/XaQ09lOn1qbgCodKVPE/3uJwbMbdf+4KjZ3v3EH5rt1S323+j5wfSa9fBhux49td1bgtv+Fbmwzp7Cev2J9ONf9L6v6+xrL7ylz3tK6dvP3TBo73vGHspcGuq1tzdb28Nq6CMRNQHDTxQUw08UFMNPFBTDTxQUw08UFMNPFFSYOj86nG91bt5srnakD3w/v8P+G9rfZ9eEB5573d5+ctJs10p6PTs/aF+D0PucPWa+etFeXtxbgluM532wz56yfOZPfs8+dLc9nn/q4+lzMMwPOBMZnHvXPvaw3XdvuvZ6li5vFJ75iYJi+ImCYviJgmL4iYJi+ImCYviJgmL4iYJy6/wicgDA5wCcUdVrk/sGATwBYDuAMQB3qapdjM6YdDjj1nO1/x1c6LdrtuUdm8z2wptnzXa59mN2B6w55mftOr63lLR02b8i3vz11vz33sz33YfH7Af029dPiHGA2Q1Onb1sL23ufd+5zvQ1AwCgOjdnH78FVvIb/y0At15y3/0ADqnq1QAOJV8T0Srihl9Vnwdw7pK7dwM4mNw+COCOBveLiJqs1te6w6p6Krl9GsBwg/pDRC1S9wd+qqoAUt9Aicg+ERkVkdES7Ovniah1ag3/hIhsAoDk/zNpD1TV/ao6oqojBdgfghBR69Qa/qcB7E1u7wXwVGO6Q0St4oZfRB4H8HMAO0XkpIjcC+AhAJ8RkRMA/jL5mohWEbfOr6p7UppubnBfmkp77PnlMWmP3y51p/+dLK+155fXnF1N1xl7zQAgfVw6AOSMWr63zjwqdr07N2fXu6vedQDz6c9Ntbtg7/vKjWa7vnjcbH/7vvTPoaud9s+sOmXPY1DttK8bkWLRbMcqqfMT0WWI4ScKiuEnCorhJwqK4ScKiuEnCirO1N0F+1vVWbv0UpgxhnAW7HKZW27zeMNmrWminVIe8k4Z0hn06w1ttfafW7DLbRWnFOgNR9aB9GW2e4/b037nh+wlvOeNqdwBQHq6zXZM2cuLtwLP/ERBMfxEQTH8REEx/ERBMfxEQTH8REEx/ERBxanzV2tfShoAcuX07bdts6fe7nzdHhaLbnu4sTq1eKvWbl4DANR9HUA9vO+rXrmz6cNq+1+3rzGY37nZbFenzq/TM2Z7O+CZnygohp8oKIafKCiGnygohp8oKIafKCiGnyioOHV+Z4nuxVXH0lWNuu5b4+vNbXeePma2y86P2u3emPlVyl3mWuxavPT1me2Dx9J/ZhVnZu2ZYfsBvb9xplvfZE87zvH8RJQZhp8oKIafKCiGnygohp8oKIafKCiGnygot84vIgcAfA7AGVW9NrnvQQBfAPDeQPYHVPWZZnWyJSrOMtvG2PPiSbsmrKX0+eMBAN5yz84y2e6YfUuTx9Sjnr45cs7c+kM/O53adnL3JnPbnrftaxA6zk2b7XCWZW8HKznzfwvArcvc/3VVvT75t7qDTxSQG35VfR7AuRb0hYhaqJ73/PeJyFEROSAiAw3rERG1RK3h/waAHQCuB3AKwFfTHigi+0RkVERGS5iv8XBE1Gg1hV9VJ1S1oqpVAN8EsMt47H5VHVHVkQI6a+0nETVYTeEXkaUfld4J4OXGdIeIWmUlpb7HAdwEYEhETgL4BwA3icj1ABTAGIAvNrGPRNQEbvhVdc8ydz/ahL4010LJbPbm7bfGf+cqddZ0nVr4aq7jW9dHeOP51amVV3vs9Q7w2lhq0/SWYXPTzklnrQTn90mLBbO9HfAKP6KgGH6ioBh+oqAYfqKgGH6ioBh+oqDiTN1dp6pRsso5Vy3nN2ywHzBt70Dz9pDf1areqbu1y/71tYp1XWft8544S7pr3jlvXiZDeonoMsTwEwXF8BMFxfATBcXwEwXF8BMFxfATBcU6/wrlKul13y5vetON9hTTMm0v96z9Pfb+jb41nbO0uVVt1w7n3OPt22nXSvp1BOKsel5xJp2SsnMNQqezBngb4JmfKCiGnygohp8oKIafKCiGnygohp8oKIafKKg4dX6vZtxpF3atunCl0xm77dWEC86PIcs6vqeOacWrnc54fGdMfW7Wnj67quk/NHVOe4Vp5zkv2cumy8ycvX0b4JmfKCiGnygohp8oKIafKCiGnygohp8oKIafKCi3zi8iWwE8BmAYgALYr6oPi8gggCcAbAcwBuAuVZ1sXlfr446/rtrtMxvT/07m55ya8Pkpu32w326vZ5ntepb3hr3Edr28Or57bUbVWeK7bNfiLfkFu93bt1wmS3SXAXxFVa8B8McAviQi1wC4H8AhVb0awKHkayJaJdzwq+opVX0puX0BwCsAtgDYDeBg8rCDAO5oVieJqPE+1Ht+EdkO4JMAXgAwrKqnkqbTWHxbQESrxIrDLyK9AL4H4Muq+r43saqqWPw8YLnt9onIqIiMluAsakdELbOi8ItIAYvB/7aqfj+5e0JENiXtmwCcWW5bVd2vqiOqOlKAMysiEbWMG34REQCPAnhFVb+2pOlpAHuT23sBPNX47hFRs6xkSO+nAdwD4JiIHEnuewDAQwD+XUTuBfAmgLua08UGmXdqN84QzdmN6WWnzT+zt63+1p7bO7durdkuC/b+7SW8nXKZN322w1tm2xyO7B3b2bfM2m8jpZA+fXbVeRHaMet8X7P2kF1d22tv3wbc8KvqT5E++frNje0OEbUKr/AjCorhJwqK4ScKiuEnCorhJwqK4ScKKs7U3XUq96XXffNzdk04v2HIbNecPWxWO+3hodawW3Gm/faG7GrePj947e6wXUN1jf19y/iE2X7hzhtS20ofmzG3XfMDZzjwmi67fRXgmZ8oKIafKCiGnygohp8oKIafKCiGnygohp8oqDB1fvWmeXaniU6vh3e+9Ia976EBs700ZI/91g5n+uw6VvD2xuOrM+a+6vTNuo4gV7Y7Xi3Yx85vsaeNXHf4VGrb1JVbzG3nh+zlv7tenTXbc8X0uQTaBc/8REEx/ERBMfxEQTH8REEx/ERBMfxEQTH8REGFqfPDW1K5yxmfnUuvSbvLNdt7duv43nh/s9buLtFtzfnvU29zq5Tv1Pm9axBKQ91me9EY7989sdncdvojdjTWeM9rR33PayvwzE8UFMNPFBTDTxQUw08UFMNPFBTDTxQUw08UlFvnF5GtAB4DMIzFqu1+VX1YRB4E8AUAZ5OHPqCqzzSro/USZ3y19to14/5fGXXbSsXctnLCHu9fON1ntmdJ3OsEmijv1Mo77F/fyuRkalvnlP0zmxmyjy099u8Lyvb+28FKLvIpA/iKqr4kIn0AXhSRZ5O2r6vqPzeve0TULG74VfUUgFPJ7Qsi8goAexoUImp7H+o9v4hsB/BJAC8kd90nIkdF5ICILDtXlYjsE5FRERktYb6uzhJR46w4/CLSC+B7AL6sqlMAvgFgB4DrsfjK4KvLbaeq+1V1RFVHCuhsQJeJqBFWFH4RKWAx+N9W1e8DgKpOqGpFVasAvglgV/O6SUSN5oZfFj/ufRTAK6r6tSX3b1rysDsBvNz47hFRs6zk0/5PA7gHwDEROZLc9wCAPSJyPRbLf2MAvtiUHjaKUxZa2LLObL+wPX346cYZe7lnT/XChbq2pw+vb/Sk2X7+7u1me3WjPR27/Ob0h+1Sy63k0/6fYvkh6W1b0yciH6/wIwqK4ScKiuEnCorhJwqK4ScKiuEnCirM1N3Vs++Y7bmt6832gVca2ZtLeMNmveXD6UMrj79tthenttW1f51v/3EsPPMTBcXwEwXF8BMFxfATBcXwEwXF8BMFxfATBSXawhqyiJwF8OaSu4YA2AX47LRr39q1XwD7VqtG9m2bqm5YyQNbGv4PHFxkVFVHMuuAoV371q79Ati3WmXVN77sJwqK4ScKKuvw78/4+JZ27Vu79gtg32qVSd8yfc9PRNnJ+sxPRBnJJPwicquIvCoir4nI/Vn0IY2IjInIMRE5IiKjGfflgIicEZGXl9w3KCLPisiJ5H97DunW9u1BERlPnrsjInJ7Rn3bKiL/JSK/FJHjIvLXyf2ZPndGvzJ53lr+sl9E8gB+DeAzAE4COAxgj6r+sqUdSSEiYwBGVDXzmrCI/DmAiwAeU9Vrk/v+CcA5VX0o+cM5oKp/2yZ9exDAxaxXbk4WlNm0dGVpAHcA+DwyfO6Mft2FDJ63LM78uwC8pqpvqOoCgO8C2J1BP9qeqj4P4Nwld+8GcDC5fRCLvzwtl9K3tqCqp1T1peT2BQDvrSyd6XNn9CsTWYR/C4C3lnx9Eu215LcC+LGIvCgi+7LuzDKGk2XTAeA0gOEsO7MMd+XmVrpkZem2ee5qWfG60fiB3wfdqKo3ALgNwJeSl7dtSRffs7VTuWZFKze3yjIrS/9Ols9drSteN1oW4R8HsHXJ11ck97UFVR1P/j8D4Em03+rDE+8tkpr8fybj/vxOO63cvNzK0miD566dVrzOIvyHAVwtIleJSBHA3QCezqAfHyAiPckHMRCRHgC3oP1WH34awN7k9l4AT2XYl/dpl5Wb01aWRsbPXduteK2qLf8H4HYsfuL/OoC/z6IPKf36KIBfJP+OZ903AI9j8WVgCYufjdwLYD2AQwBOAHgOwGAb9e3fABwDcBSLQduUUd9uxOJL+qMAjiT/bs/6uTP6lcnzxiv8iILiB35EQTH8REEx/ERBMfxEQTH8REEx/ERBMfxEQTH8REH9PxN052IM1+5yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "valid_data = FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "\n",
    "train_idx = np.random.choice(train_data.train_data.shape[0], 54000, replace=False)\n",
    "\n",
    "train_data.train_data = train_data.train_data[train_idx, :]\n",
    "train_data.train_labels = train_data.train_labels[torch.from_numpy(train_idx).type(torch.LongTensor)]\n",
    "\n",
    "mask = np.ones(60000)\n",
    "mask[train_idx] = 0\n",
    "\n",
    "valid_data.train_data = valid_data.train_data[torch.from_numpy(np.argwhere(mask)), :].squeeze()\n",
    "valid_data.train_labels = valid_data.train_labels[torch.from_numpy(mask).type(torch.ByteTensor)]\n",
    "\n",
    "batch_size = 100\n",
    "test_batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    FashionMNIST('../data', train=False, transform=torchvision.transforms.Compose([\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "validation_data = {}\n",
    "loss_train_data = {}\n",
    "loss_validation_data = {}\n",
    "\n",
    "plt.imshow(train_loader.dataset.train_data[1].numpy()) #Potentially printing out an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe022c05438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_loader.dataset.train_data[10].numpy()) # another image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 1: FCC sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FccWithoutDropout(nn.Module):\n",
    "    def __init__(self, model_id, input_size, hidden_sizes, output_size, activation_function):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.activation_function = activation_function\n",
    "        self.inputToHidden = nn.Linear(input_size, hidden_sizes[0]) #Linear(D, M)\n",
    "        self.hiddenToHidden = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hiddenToHidden.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        self.hiddenToOutput = nn.Linear(hidden_sizes[len(hidden_sizes) - 1], output_size) #Linear(M, K)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        batch_size = image.size()[0]\n",
    "        x = image.view(batch_size, -1)\n",
    "        x = self.activation_function(self.inputToHidden(x))\n",
    "        for f in self.hiddenToHidden:\n",
    "            x = self.activation_function(f(x))\n",
    "        x = F.log_softmax(self.hiddenToOutput(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 2: FCC avec dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FccWithDropout(nn.Module):\n",
    "    def __init__(self, model_id, input_size, hidden_sizes, output_size, activation_function):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.activation_function = activation_function\n",
    "        self.inputToHidden = nn.Linear(input_size, hidden_sizes[0]) #Linear(D, M)\n",
    "        self.hiddenToHidden = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hiddenToHidden.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        self.hiddenToOutput = nn.Linear(hidden_sizes[len(hidden_sizes) - 1], output_size) #Linear(M, K)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        batch_size = image.size()[0]\n",
    "        x = image.view(batch_size, -1)\n",
    "        x = self.activation_function(nn.Dropout(self.inputToHidden(x)))\n",
    "        for f in self.hiddenToHidden:\n",
    "            x = self.activation_function(nn.Dropout(f(x)))\n",
    "        x = F.log_softmax(nn.Dropout(self.hiddenToOutput(x)), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 3: CNN sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCnnWithoutDropout(nn.Module): #taken from https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/\n",
    "    #Our batch shape for input x is (1, 28, 28)\n",
    "    \n",
    "    def __init__(self, model_id):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        #Input channels = 1, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(1, 25, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(25 * 14 * 14, 256)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (1, 28, 28) to (25, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (25, 28, 28) to (25, 14, 14)\n",
    "        x = self.pool(x)\n",
    "       \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (25, 14, 14) to (1, 4900)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 25 * 14 *14)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4900) to (1, 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 256) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 4: CNN avec dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCnnWithDropout(nn.Module): #taken from https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/\n",
    "    #Our batch shape for input x is (1, 28, 28)\n",
    "    \n",
    "    def __init__(self, model_id):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        #Input channels = 1, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(1, 25, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(25 * 14 * 14, 256)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (1, 28, 28) to (25, 28, 28)\n",
    "        x = F.relu(nn.Dropout(self.conv1(x)))\n",
    "        \n",
    "        #Size changes from (25, 28, 28) to (25, 14, 14)\n",
    "        x = self.pool(x)\n",
    "       \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (25, 14, 14) to (1, 4900)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 25 * 14 *14)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4900) to (1, 256)\n",
    "        x = F.relu(nn.Dropout(self.fc1(x)))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 256) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(nn.Dropout(x), dim = 1)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 5: VGG sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 6: VGG avec dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 7: Inception V3 sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 8: Inception V3 avec dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 9: Resnet sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 10: Resnet avec dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #data, target = Variable(data, volatile=True).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # calls the forward function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    loss_train_data[model.model_id].append(total_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        #data, target = Variable(data, volatile=True).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        valid_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    print('\\n' + \"valid\" + ' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        valid_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))\n",
    "    loss_validation_data[model.model_id].append(valid_loss)\n",
    "    return 1.0 * correct.item() / len(valid_loader.dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = Variable(data, volatile=True).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\n' + \"test\" + ' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expérimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, epochs=10, lr=0.001):\n",
    "    best_precision = 0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    validation_data[model.model_id] = []\n",
    "    loss_validation_data[model.model_id]= []\n",
    "    loss_train_data[model.model_id] = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model = train(model, train_loader, optimizer)\n",
    "        precision = valid(model, valid_loader)\n",
    "        validation_data[model.model_id].append(precision)\n",
    "        if precision > best_precision:\n",
    "            best_precision = precision\n",
    "            best_model = model\n",
    "    return best_model, best_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid set: Average loss: 0.3912, Accuracy: 5156/6000 (85%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.3439, Accuracy: 5250/6000 (87%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.3342, Accuracy: 5262/6000 (87%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.3170, Accuracy: 5292/6000 (88%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.2958, Accuracy: 5324/6000 (88%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.3029, Accuracy: 5312/6000 (88%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.3060, Accuracy: 5298/6000 (88%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.3139, Accuracy: 5300/6000 (88%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.2956, Accuracy: 5329/6000 (88%)\n",
      "\n",
      "\n",
      "valid set: Average loss: 0.2955, Accuracy: 5366/6000 (89%)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-950c617f6c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m               SimpleCnnWithoutDropout(\"cnn1\"), SimpleCnnWithDropout(\"cnn2\")]:  # add your models in the list\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#model.cuda()  # if you have access to a gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_precision\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbest_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-677c6a9f2137>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-50b1f3f31f9b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calls the forward function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2a22a01bdedf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputToHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddenToHidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, p, inplace)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DropoutNd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             raise ValueError(\"dropout probability has to be between 0 and 1, \"\n\u001b[1;32m     13\u001b[0m                              \"but got {}\".format(p))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "best_precision = 0\n",
    "#SimpleCnn(\"CNN\"), \n",
    "for model in [FccWithoutDropout(\"fcc1\",28*28, [512], 10, F.sigmoid), \n",
    "              FccWithDropout(\"fcc2\",28*28, [512], 10, F.sigmoid), \n",
    "              SimpleCnnWithoutDropout(\"cnn1\"), SimpleCnnWithDropout(\"cnn2\")]:  # add your models in the list\n",
    "    #model.cuda()  # if you have access to a gpu\n",
    "    model, precision = experiment(model)\n",
    "    if precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_model = model\n",
    "\n",
    "test(best_model, test_loader)\n",
    "print(\"The best model is: \" + best_model.model_id)\n",
    "for i, data in validation_data.items():\n",
    "    plt.plot(data, label = i)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Different Acivation Functions\")\n",
    "plt.show()\n",
    "\n",
    "for i, data in loss_validation_data.items():\n",
    "    plt.plot(data, label = i+\" validating\")\n",
    "for i, data in loss_train_data.items():\n",
    "    plt.plot(data, label = i+\" training\", linestyle = '--')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
