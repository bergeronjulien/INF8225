{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout is best !!!!111!1!1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from fashion import FashionMNIST\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d''Inception3', \n",
    "           'inception_v3',\n",
    "           'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19',\n",
    "           'ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    # Inception v3 ported from TensorFlow\n",
    "    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
    "    # vgg with TensorFlow\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
    "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
    "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
    "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
    "    # ResNet with TensorFlow\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "_InceptionOuputs = namedtuple('InceptionOuputs', ['logits', 'aux_logits'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b552560470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       #torchvision.transforms.Grayscale(3),\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "valid_data = FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       #torchvision.transforms.Grayscale(3),\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "\n",
    "train_idx = np.random.choice(train_data.data.shape[0], 54000, replace=False)\n",
    "\n",
    "train_data.data = train_data.data[train_idx, :]\n",
    "train_data.targets = train_data.targets[torch.from_numpy(train_idx).type(torch.LongTensor)]\n",
    "\n",
    "mask = np.ones(60000)\n",
    "mask[train_idx] = 0\n",
    "\n",
    "valid_data.data = valid_data.data[torch.from_numpy(np.argwhere(mask)), :].squeeze()\n",
    "valid_data.targets = valid_data.targets[torch.from_numpy(mask).type(torch.ByteTensor)]\n",
    "\n",
    "batch_size = 100\n",
    "test_batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    FashionMNIST('../data', train=False, transform=torchvision.transforms.Compose([\n",
    "                       torchvision.transforms.Grayscale(3),\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "validation_data = {}\n",
    "loss_train_data = {}\n",
    "loss_validation_data = {}\n",
    "\n",
    "plt.imshow(train_loader.dataset.data[1].numpy()) #Potentially printing out an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b5532945c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_loader.dataset.data[10].numpy()) # another image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 1: FCC sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FccWithoutDropout(nn.Module):\n",
    "    def __init__(self, model_id, input_size, hidden_sizes, output_size, activation_function):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.activation_function = activation_function\n",
    "        self.inputToHidden = nn.Linear(input_size, hidden_sizes[0]) #Linear(D, M)\n",
    "        self.hiddenToHidden = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hiddenToHidden.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        self.hiddenToOutput = nn.Linear(hidden_sizes[len(hidden_sizes) - 1], output_size) #Linear(M, K)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        batch_size = image.size()[0]\n",
    "        x = image.view(batch_size, -1)\n",
    "        x = self.activation_function(self.inputToHidden(x))\n",
    "        for f in self.hiddenToHidden:\n",
    "            x = self.activation_function(f(x))\n",
    "        x = F.log_softmax(self.hiddenToOutput(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 2: FCC avec dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FccWithDropout(nn.Module):\n",
    "    def __init__(self, model_id, input_size, hidden_sizes, output_size, activation_function, dropout):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.activation_function = activation_function\n",
    "        self.inputToHidden = nn.Linear(input_size, hidden_sizes[0]) #Linear(D, M)\n",
    "        self.hiddenToHidden = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hiddenToHidden.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        self.hiddenToOutput = nn.Linear(hidden_sizes[len(hidden_sizes) - 1], output_size) #Linear(M, K)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, image):\n",
    "        batch_size = image.size()[0]\n",
    "        x = image.view(batch_size, -1)\n",
    "        x = self.activation_function(self.dropout(self.inputToHidden(x)))\n",
    "        for f in self.hiddenToHidden:\n",
    "            x = self.activation_function(self.dropout(f(x)))\n",
    "        x = F.log_softmax(self.dropout(self.hiddenToOutput(x)), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 3: CNN sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCnnWithoutDropout(nn.Module): #taken from https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/\n",
    "    #Our batch shape for input x is (1, 28, 28)\n",
    "    \n",
    "    def __init__(self, model_id):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        #Input channels = 1, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(1, 25, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(25 * 14 * 14, 256)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (1, 28, 28) to (25, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (25, 28, 28) to (25, 14, 14)\n",
    "        x = self.pool(x)\n",
    "       \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (25, 14, 14) to (1, 4900)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 25 * 14 *14)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4900) to (1, 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 256) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 4: CNN avec dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCnnWithDropout(nn.Module): #taken from https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/\n",
    "    #Our batch shape for input x is (1, 28, 28)\n",
    "    \n",
    "    def __init__(self, model_id, dropout):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #Input channels = 1, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(1, 25, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(25 * 14 * 14, 256)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (1, 28, 28) to (25, 28, 28)\n",
    "        x = F.relu(self.dropout(self.conv1(x)))\n",
    "        \n",
    "        #Size changes from (25, 28, 28) to (25, 14, 14)\n",
    "        x = self.pool(x)\n",
    "       \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (25, 14, 14) to (1, 4900)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 25 * 14 *14)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4900) to (1, 256)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 256) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(self.dropout(x), dim = 1)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèles 5 et 6: VGG avec et sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, model_id, dropout=True, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.model_id = model_id\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        if dropout:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(512 * 7 * 7, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, num_classes),\n",
    "            )\n",
    "        else :\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(512 * 7 * 7, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(4096, num_classes),\n",
    "            )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 1\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def vgg11(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['A']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg13(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['B']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg13']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg13_bn']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['D']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg16']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg19(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['E']), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, **kwargs):\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèles 7 et 8: Inception V3 avec et sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3(pretrained=False, dropout=True, **kwargs):\n",
    "    r\"\"\"Inception v3 model architecture from\n",
    "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
    "    .. note::\n",
    "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
    "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n",
    "            Default: *True*\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        if 'aux_logits' in kwargs:\n",
    "            original_aux_logits = kwargs['aux_logits']\n",
    "            kwargs['aux_logits'] = True\n",
    "        else:\n",
    "            original_aux_logits = True\n",
    "        model = Inception3(**kwargs)\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['inception_v3_google']))\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            del model.AuxLogits\n",
    "        return model\n",
    "    if dropout:\n",
    "        return Inception3WithDropout(**kwargs)\n",
    "    else :\n",
    "        return Inception3WithoutDropout(**kwargs)\n",
    "\n",
    "\n",
    "class Inception3WithDropout(nn.Module):\n",
    "\n",
    "    def __init__(self, model_id, num_classes=1000, aux_logits=True, transform_input=False):\n",
    "        super(Inception3WithDropout, self).__init__()\n",
    "        self.model_id = model_id\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(1, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
    "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
    "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
    "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
    "        self.Mixed_6a = InceptionB(288)\n",
    "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
    "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = InceptionAux(768, num_classes)\n",
    "        self.Mixed_7a = InceptionD(768)\n",
    "        self.Mixed_7b = InceptionE(1280)\n",
    "        self.Mixed_7c = InceptionE(2048)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        # N x 1 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        if self.training and self.aux_logits:\n",
    "            aux = self.AuxLogits(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 2048\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        if self.training and self.aux_logits:\n",
    "            return _InceptionOuputs(x, aux)\n",
    "        return x\n",
    "\n",
    "class Inception3WithoutDropout(nn.Module):\n",
    "\n",
    "    def __init__(self, model_id, num_classes=1000, aux_logits=True, transform_input=False):\n",
    "        super(Inception3WithoutDropout, self).__init__()\n",
    "        self.model_id = model_id\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(1, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
    "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
    "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
    "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
    "        self.Mixed_6a = InceptionB(288)\n",
    "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
    "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = InceptionAux(768, num_classes)\n",
    "        self.Mixed_7a = InceptionD(768)\n",
    "        self.Mixed_7b = InceptionE(1280)\n",
    "        self.Mixed_7c = InceptionE(2048)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        # N x 1 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        if self.training and self.aux_logits:\n",
    "            aux = self.AuxLogits(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 2048 x 1 x 1 This is the dropout line that needs to be commented out.\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 2048\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        if self.training and self.aux_logits:\n",
    "            return _InceptionOuputs(x, aux)\n",
    "        return x\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pool_features):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionB(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionB, self).__init__()\n",
    "        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, channels_7x7):\n",
    "        super(InceptionC, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionD(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionD, self).__init__()\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionE, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n",
    "        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n",
    "        self.conv1.stddev = 0.01\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.fc.stddev = 0.001\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 768 x 17 x 17\n",
    "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
    "        # N x 768 x 5 x 5\n",
    "        x = self.conv0(x)\n",
    "        # N x 128 x 5 x 5\n",
    "        x = self.conv1(x)\n",
    "        # N x 768 x 1 x 1\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 768 x 1 x 1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 768\n",
    "        x = self.fc(x)\n",
    "        # N x 1000\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèles 9 et 10: Resnet avec et sans dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, groups=groups, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlockWithDropout(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, norm_layer=None):\n",
    "        super(BasicBlockWithDropout, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.dropout(out, training=self.training);\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.dropout(out, training=self.training);\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class BasicBlockWithoutDropout(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, norm_layer=None):\n",
    "        super(BasicBlockWithoutDropout, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetWithDropout(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, model_id, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, norm_layer=None):\n",
    "        super(ResNetWithDropout, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            \n",
    "        self.model_id = model_id\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None):\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResNetWithoutDropout(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, model_id, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, norm_layer=None):\n",
    "        super(ResNetWithoutDropout, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            \n",
    "        self.model_id = model_id\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None):\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet18(pretrained=False, dropout=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        dropout (bool): If True, returns a model using dropout\n",
    "    \"\"\"\n",
    "    if dropout:\n",
    "        model = ResNetWithDropout(BasicBlockWithDropout, [2, 2, 2, 2], **kwargs)\n",
    "    else :\n",
    "        model = ResNetWithoutDropout(BasicBlockWithoutDropout, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data, volatile=True).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # calls the forward function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    loss_train_data[model.model_id].append(total_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        data, target = Variable(data, volatile=True).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        valid_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    print('\\n' + \"valid\" + ' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        valid_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))\n",
    "    loss_validation_data[model.model_id].append(valid_loss)\n",
    "    return 1.0 * correct.item() / len(valid_loader.dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True).cuda(), Variable(target).cuda() # if you have access to a gpu\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\n' + \"test\" + ' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expérimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, epochs=10, lr=0.001):\n",
    "    best_precision = 0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    validation_data[model.model_id] = []\n",
    "    loss_validation_data[model.model_id]= []\n",
    "    loss_train_data[model.model_id] = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model = train(model, train_loader, optimizer)\n",
    "        precision = valid(model, valid_loader)\n",
    "        validation_data[model.model_id].append(precision)\n",
    "        if precision > best_precision:\n",
    "            best_precision = precision\n",
    "            best_model = model\n",
    "    return best_model, best_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition des modèles utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "halfDropout = nn.Dropout(0.5)\n",
    "fifthDropout = nn.Dropout(0.2)\n",
    "\n",
    "fcc = FccWithoutDropout(\"fcc\",28*28, [512], 10, F.sigmoid)\n",
    "fccHalfDropout = FccWithDropout(\"fcc_d.5\",28*28, [512], 10, F.sigmoid, halfDropout)\n",
    "fccFifthDropout = FccWithDropout(\"fcc_d.2\",28*28, [512], 10, F.sigmoid, fifthDropout)\n",
    "cnn = SimpleCnnWithoutDropout(\"cnn\")\n",
    "cnnHalfDropout = SimpleCnnWithDropout(\"cnn_d.5\", halfDropout)\n",
    "cnnFifthDropout = SimpleCnnWithDropout(\"cnn_d.2\", fifthDropout)\n",
    "vggWithoutDropout = vgg16(dropout=False, model_id = \"vgg\", num_classes=10)\n",
    "vggWithDropout = vgg16(dropout=True, model_id = \"vgg_d\", num_classes=10)\n",
    "inceptionWithoutDropout = inception_v3(dropout=False, model_id = \"inception\", num_classes=10)\n",
    "inceptionWithDropout = inception_v3(dropout=True, model_id = \"inception_d\", num_classes=10)\n",
    "resNetWithoutDropout = resnet18(dropout=False, model_id = \"resnet\", num_classes=10)\n",
    "resNetWithDropout = resnet18(dropout=True, model_id = \"resnet_d\", num_classes=10)\n",
    "\n",
    "models = [\n",
    "          fcc, \n",
    "          fccHalfDropout, \n",
    "          fccFifthDropout,\n",
    "          cnn,\n",
    "          cnnHalfDropout,\n",
    "          cnnFifthDropout, \n",
    "          vggWithoutDropout,\n",
    "          vggWithDropout, \n",
    "          inceptionWithoutDropout,\n",
    "          inceptionWithDropout,\n",
    "          resNetWithoutDropout,\n",
    "          resNetWithDropout\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_precision = 0\n",
    "for model in models:\n",
    "    model.cuda()  # if you have access to a gpu\n",
    "    model, precision = experiment(model, epochs=10)\n",
    "    print(\"final precision for model \", model.model_id, \" is \", precision)\n",
    "    if precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_model = model\n",
    "\n",
    "test(best_model, test_loader)\n",
    "print(\"The best model is: \" + best_model.model_id)\n",
    "for i, data in validation_data.items():\n",
    "    plt.plot(data, label = i)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Different Acivation Functions\")\n",
    "plt.show()\n",
    "\n",
    "for i, data in loss_validation_data.items():\n",
    "    plt.plot(data, label = i+\" validating\")\n",
    "for i, data in loss_train_data.items():\n",
    "    plt.plot(data, label = i+\" training\", linestyle = '--')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
